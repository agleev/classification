# Задачи классификации
Привет!
В этом репозитории лежат мои первые проекты по классификации данных, которые я загружал на [kaggle](https://www.kaggle.com/agleev). Я брал данные, делал проект в Jupiter Notebook и выгружал на сайт. Только после этого я решил загрузить сюда, на github. В них нет контроля версий, нет отдельных файлов с заданными функциями и даже нет файла readme. Честно признаться, и некоторые проекты имеют недостатки, которые я увидел лишь спустя время.

Как оказалось потом, большая часть данных по задачам классификации и регрессии - искусственные, не очень интересно работать с ними дальше. Если прогнать их по алгоритмам с дефолтными настройками, то они покажут отличные показатели метрик качества даже без предобработки данных.

Ниже расскажу о каждом проекте и недостатках, которые в нем есть. Отдельная папка - отдельный проект с файлом данных .csv и файлом проекта.

## 01_xgboost_with_parameter_tuning

[Diabetes Health Indicators Dataset](https://www.kaggle.com/alexteboul/diabetes-health-indicators-dataset) - набор данных с показателями здоровья и наличием и остуствием диабета. Данные собирались в ходе телефонного опроса американцев, одни переменные - это прямые ответы на вопросы, другие - расчетные переменные, которые были основаны на ответах. Датасет не имеет пропусков и сбалансирован (таргет - 50% на 50%).

1. Я разделил переменные на непрерывные и категориальные.
2. Категориальные переменные я разбил на отдельные признаки со значением - 1 и 0.
3. Непрерыную переменную с индексом массы тела `bmi` я заменил на группы, исходя из [общепринятой таблицы значений](https://ru.wikipedia.org/wiki/%D0%98%D0%BD%D0%B4%D0%B5%D0%BA%D1%81_%D0%BC%D0%B0%D1%81%D1%81%D1%8B_%D1%82%D0%B5%D0%BB%D0%B0).
4. Прошкалировал непрерывные переменные.
5. Запустил алгоритм `XGBClassifier` на дефолтных настройках, чтобы было с чем сравнить.
6. Задал функцию обучения, с перекрестной проверкой и автоматическим поиском параметров. Настроил ноутбук так, чтобы можно было просто запустить и все.
7. Алгоритм сначала подобрал значение `n_estimators` и `learning_rate` потом снова перепроверил, но уже на соседних значениях. И так по остальным гиперпараметрам: `max_depth`, `min_child_weight`, `subsample`, `colsample_bytree`, `gamma`. С каджды последующим шагом они автоматически добавлялись в классификатор.

[Diabetes Health Indicators Dataset](https://www.kaggle.com/agleev/xgboost-with-parameter-tuning) - этот проект на kaggle.
